{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run below code in case of LookupError, like: \n",
    "# \"Resource punkt not found. Please use the NLTK Downloader to obtain the resource:\"\n",
    "# \"Resource stopwords not found. Please use the NLTK Downloader to obtain the resource:\"\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# In Windows 10, the data is downloaded to C:\\Users\\username\\AppData\\Roaming\\nltk_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data_file():\n",
    "    try:\n",
    "        train_file = io.open('data/kaggle/1-sms-spam-train.txt', encoding='utf-8')\n",
    "    except:\n",
    "        print('Error reading training file..')\n",
    "\n",
    "    lines = train_file.readlines()\n",
    "    train_file.close()\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def process_train_data():\n",
    "    lines = load_train_data_file()\n",
    "    maxsplit=2\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        data.append(line.split('\\t', maxsplit=maxsplit))\n",
    "    return data\n",
    "\n",
    "xtrain = pd.DataFrame(data=process_train_data(), columns=['target','sms'])\n",
    "xtrain = xtrain.apply(lambda s : s.str.strip())\n",
    "xtrain = xtrain[['sms','target']]\n",
    "print(xtrain.shape)\n",
    "print(xtrain['target'].value_counts())\n",
    "xtrain.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrain.iloc[9].sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if('target' in xtrain.columns):\n",
    "    ytrain = xtrain.pop('target')\n",
    "print(ytrain.head())\n",
    "print(xtrain.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = io.open('data/kaggle/1-sms-spam-test.txt', encoding='utf-8')\n",
    "lines = test_file.readlines()\n",
    "test_file.close()\n",
    "xtest = pd.DataFrame(data=lines, columns=['sms'])\n",
    "xtest = xtest.apply(lambda s : s.str.strip())\n",
    "print(xtest.shape)\n",
    "xtest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_rows = xtrain.loc[ytrain=='spam']\n",
    "spam_msgs = list(spam_rows.sms)\n",
    "spam_words = ' '.join(spam_msgs)\n",
    "spam_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import STOPWORDS, WordCloud\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"co\")\n",
    "stopwords.add(\"uk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_wc = WordCloud(width=1024, height=512).generate(spam_words)\n",
    "\n",
    "plt.figure(figsize=(12,4), facecolor='k') # facecolor implies background color\n",
    "plt.imshow(spam_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the mask image\n",
    "image_skull = np.array(Image.open('image/skull_crossbones.jpg'))\n",
    "spam_wc = WordCloud(width=1024, height=512, \n",
    "                    max_words=100, \n",
    "                    background_color='#b2beb5', #'#c2b280',\n",
    "                    mask=image_skull, \n",
    "                    stopwords=stopwords)\n",
    "\n",
    "# generate word cloud\n",
    "spam_wc.generate(spam_words)\n",
    "\n",
    "# store to file\n",
    "spam_wc.to_file('data/kaggle/outputs/skull_crossbones.png')\n",
    "\n",
    "#show image\n",
    "plt.figure()\n",
    "plt.imshow(spam_wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.imshow(image_skull, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "Before starting with training we must preprocess the messages.\n",
    "* Make all characters lowercase\n",
    "* Do stemming so that words like 'go', 'goes', 'gone', etc all mean the same \n",
    "* Remove STOP words, like 'so', 'to', etc.\n",
    "* Optionally, use N-Grams to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import ngrams, everygrams, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer # Replacement to word_tokenize, that does not split contractions like - don't, isn't etc\n",
    "\n",
    "def cleanse_message(message, lower_case=True, stem=True, stop_words=True):\n",
    "    # Remove Periods\n",
    "    #message = message.replace('.',' ')\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    message = message.translate(table)\n",
    "    \n",
    "    # Convert to lower-case\n",
    "    if lower_case:\n",
    "        message = message.lower()\n",
    "    \n",
    "    # Tokenize a string to split off punctuation other than periods (The NLTK lib way)\n",
    "    #words = word_tokenize(message)\n",
    "    tt = TweetTokenizer()\n",
    "    words = tt.tokenize(message)\n",
    "    \n",
    "    # Filter by minimum word length\n",
    "#     if(min_word_length>1):\n",
    "#         words = [w for w in words if len(w)>=min_word_length]\n",
    "    \n",
    "    # Discard  STOP words\n",
    "    if(stop_words):\n",
    "        sw = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in sw]\n",
    "    \n",
    "    # Do Stemming\n",
    "    if(stem):\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # N-grams for better semantics\n",
    "#     if(gram>1):\n",
    "#         words = everygrams(words, 1,gram) # t to \"gram\" N-grams\n",
    "#         words = list(words)\n",
    "#         words = np.asarray(words)\n",
    "    message = ' '.join(w for w in words)\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "xtrain_words = list( map(lambda msg : cleanse_message(msg), \n",
    "    xtrain.sms) )\n",
    "xtrain_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidvec =  TfidfVectorizer()\n",
    "tfidvec.fit(xtrain_words,ytrain)\n",
    "res = tfidvec.transform(xtrain_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MessageCleanser(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" TODO Document\"\"\"\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        #print(type(x))\n",
    "        return list( map(lambda msg : cleanse_message(msg), x) )  \n",
    "\n",
    "# Testing    \n",
    "MessageCleanser().transform(xtrain.sms)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('msg_clnsr', MessageCleanser()),\n",
    "    ('ifidvec', TfidfVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# def fit_predict_proba(clf, xtrain, ytrain, xtest):\n",
    "#     clf.fit(xtrain, ytrain)\n",
    "#     proba = clf.predict_proba(xtest)\n",
    "#     return proba\n",
    "    \n",
    "# def save_proba(proba, fname='spam_proba.csv'):\n",
    "#     pp = pd.DataFrame(data=proba[:,1], columns=['Label'])\n",
    "#     pp = pp.round(2)\n",
    "#     pp.to_csv('data/outputs/{0}'.format(fname), header=['Label'], index_label=['Id'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(xtrain.sms, ytrain)\n",
    "preds = clf.predict(xtest.sms)\n",
    "op = pd.DataFrame(data=preds, columns=['target'])\n",
    "print(op.target.value_counts())\n",
    "'''\n",
    "ham     2329\n",
    "spam     245\n",
    "Name: target, dtype: int64\n",
    "'''\n",
    "op.to_csv('data/kaggle/outputs/predictions_mnb.csv', header=['Label'], index_label=['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = clf.predict_proba(xtest.sms)\n",
    "pp = pd.DataFrame(data=proba[:,1], columns=['Label'])\n",
    "pp = pp.round(2)\n",
    "pp.to_csv('data/kaggle/outputs/spam_proba_mnb.csv', header=['Label'], index_label=['Id'])\n",
    "# Private Score in Kaggle Leader Board : 0.98727\n",
    "# Public Score in Kaggle Leader Board : 0.96845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = Pipeline([\n",
    "    ('msg_clnsr', MessageCleanser()),\n",
    "    ('ifidvec', TfidfVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "knn_clf.fit(xtrain.sms, ytrain)\n",
    "proba = knn_clf.predict_proba(xtest.sms)\n",
    "pp = pd.DataFrame(data=proba[:,1], columns=['Label'])\n",
    "pp = pp.round(2)\n",
    "pp.to_csv('data/kaggle/outputs/spam_proba_knn.csv', header=['Label'], index_label=['Id'])\n",
    "# Private Score in Kaggle Leader Board : 0.94023\n",
    "# Public Score in Kaggle Leader Board : 0.91708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression + Ridge regularization\n",
    "from sklearn.linear_model import Ridge\n",
    "rdg_clf = Pipeline([\n",
    "    ('msg_clnsr', MessageCleanser()),\n",
    "    ('ifidvec', TfidfVectorizer()),\n",
    "    ('rdg', Ridge())\n",
    "])\n",
    "\n",
    "# Because  Ridge doesn't take string as taraget, we convert it to numeric\n",
    "ytrain_numeric = ytrain.apply(lambda y : 1 if y=='spam' else 0)\n",
    "rdg_clf.fit(xtrain.sms, ytrain_numeric)\n",
    "\n",
    "# Bacause Ridge has no attribute `predict_proba`\n",
    "# proba = rdg_clf.predict_proba(xtest.sms)\n",
    "proba = rdg_clf.predict(xtest.sms)\n",
    "pp = pd.DataFrame(data=proba, columns=['Label'])\n",
    "pp = pp.round(2)\n",
    "pp.to_csv('data/kaggle/outputs/spam_proba_rdg.csv', header=['Label'], index_label=['Id'])\n",
    "# Private Score in Kaggle Leader Board : 0.99226\n",
    "# Public Score in Kaggle Leader Board : 0.98424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
